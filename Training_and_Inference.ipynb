{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wd33jdGVtWJh"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install datasets\n",
        "!pip install tokenizers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/AkmOleksandr/picoGPT.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RqiXDIket2to",
        "outputId": "f5c468ef-3186-4ba0-dbe2-468bedabdb9d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'picoGPT'...\n",
            "remote: Enumerating objects: 203, done.\u001b[K\n",
            "remote: Counting objects: 100% (33/33), done.\u001b[K\n",
            "remote: Compressing objects: 100% (23/23), done.\u001b[K\n",
            "remote: Total 203 (delta 17), reused 25 (delta 10), pack-reused 170\u001b[K\n",
            "Receiving objects: 100% (203/203), 35.75 KiB | 2.98 MiB/s, done.\n",
            "Resolving deltas: 100% (117/117), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "%cd /content/picoGPT"
      ],
      "metadata": {
        "id": "_HcHzXKVt5Fd"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "print(\"Current Directory:\", os.getcwd())\n",
        "print(\"Directory Contents:\", os.listdir())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-lXBZWkvA7M",
        "outputId": "e968d3c3-5385-45ba-b1eb-0c41168e84ab"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current Directory: /content/picoGPT\n",
            "Directory Contents: ['.git', 'README.md', 'inference.py', 'model.py', 'train.py', 'config.py', 'dataset.py']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "wo6SavCKvGUo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/drive/MyDrive/Models/picoGPT/weights\n",
        "!mkdir -p /content/drive/MyDrive/Models/picoGPT/vocab"
      ],
      "metadata": {
        "id": "YKpdpuFJvJGN"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from config import get_config\n",
        "\n",
        "config = get_config()\n",
        "\n",
        "config['model_folder'] = '..//drive/MyDrive/Models/picoGPT/weights'\n",
        "config['tokenizer_file'] = '..//drive/MyDrive/Models/picoGPT/vocab/tokenizer.json'\n",
        "config['batch_size'] = 16\n",
        "config['num_epochs'] = 7"
      ],
      "metadata": {
        "id": "IkkwIuQTvQgX"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "rsZ4Hdo9EO74"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from train import train_model\n",
        "\n",
        "train_model(config)"
      ],
      "metadata": {
        "id": "FDRVnTMTvhMT"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference"
      ],
      "metadata": {
        "id": "oAEoCpsuEWF5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from inference import get_response"
      ],
      "metadata": {
        "id": "puw9iZ-boY3Y"
      },
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_response(config, \"People should know that AI\", temperature=0.7, top_p=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "z-t28dROoP_t",
        "outputId": "06ffb079-9182-4f66-daec-d1e131c56c5d"
      },
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'is a reliable tool for improving the quality of life .'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 213
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pretty Inference"
      ],
      "metadata": {
        "id": "xa_LQTBYpR7m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install colorama"
      ],
      "metadata": {
        "id": "tTyju1OvDlR4"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Style\n",
        "from colorama import Fore, Style, Back\n",
        "import textwrap\n",
        "import re\n",
        "# Random selection of optional hyperparameters\n",
        "import random\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "5kynLnBWWAWC"
      },
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _remove_spaces_before_sc(text): # remove spaces before special characters\n",
        "  return re.sub(r'\\s+([^\\w\\s])', r'\\1', text)"
      ],
      "metadata": {
        "id": "t-oG2LuSgTtp"
      },
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pretty_response(prompts):\n",
        "  print(f\"The prompt is {Fore.BLUE}blue{Style.RESET_ALL} and the model's output is {Fore.GREEN}green{Style.RESET_ALL}\")\n",
        "  print(\"--\" * 80)\n",
        "  print(\"Examples:\\n\")\n",
        "  for prompt in prompts:\n",
        "      temperature = np.random.normal(0.5, 0.01)  # sample from normal distribution\n",
        "      top_p = random.choice([10, 15, 20, None])   # select random item from the list\n",
        "      output = get_response(config, prompt, temperature=temperature, top_p=top_p)\n",
        "\n",
        "      # Format the prompt and output with appropriate colors\n",
        "      formatted_prompt = f\"{Fore.BLUE}{prompt}{Style.RESET_ALL}\"\n",
        "      formatted_output = f\"{Fore.GREEN}{output}{Style.RESET_ALL}\"\n",
        "\n",
        "      # Wrap the lines to fit within the specified width and remove spaces before special characters\n",
        "      wrapped_prompt = _remove_spaces_before_sc(textwrap.fill(formatted_prompt, width=120))\n",
        "      wrapped_output = _remove_spaces_before_sc(textwrap.fill(formatted_output, width=120))\n",
        "\n",
        "      print(f\"{wrapped_prompt} {wrapped_output}\\n\")"
      ],
      "metadata": {
        "id": "FLS4gT6aDFeb"
      },
      "execution_count": 195,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompts =[\"The main problem in medicine is\", \"The biggest issue in the world is\", \"New technology is\", \"People who play sports are\"]\n",
        "get_pretty_response(prompts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmA-ordpps4Z",
        "outputId": "7f3ca35c-d300-409c-b96d-f1b362333b36"
      },
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The prompt is \u001b[34mblue\u001b[0m and the model's output is \u001b[32mgreen\u001b[0m\n",
            "------------------------------------------------------------------------------------------------------------------------------\n",
            "Examples:\n",
            "\n",
            "\u001b[34mThe main problem in medicine is\u001b[0m \u001b[32mthe lack of a comprehensive understanding of the pathophysiology of cancer.\u001b[0m\n",
            "\n",
            "\u001b[34mThe biggest issue in the world is\u001b[0m \u001b[32mthe development of a new method of improving the quality of the education system.\u001b[0m\n",
            "\n",
            "\u001b[34mNew technology is\u001b[0m \u001b[32mbeing developed in the field of medical education and training.\u001b[0m\n",
            "\n",
            "\u001b[34mPeople who play sports are\u001b[0m \u001b[32mmore likely to have a more general experience than those who are not in the sports.\u001b[0m\n",
            "\n"
          ]
        }
      ]
    }
  ]
}
